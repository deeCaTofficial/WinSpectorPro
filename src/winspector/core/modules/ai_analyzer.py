# src/winspector/core/modules/ai_analyzer.py
"""
–ú–æ–¥—É–ª—å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å API Google Generative AI (Gemini).
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –ø—Ä–∏–Ω—è—Ç–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–ª–∞–Ω–æ–≤,
–æ—Ç—á–µ—Ç–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é.
"""

import os
import json
import logging
import time
import hashlib
import re
import google.generativeai as genai
from typing import Dict, Any, List, Tuple

logger = logging.getLogger(__name__)


class AIAnalyzer:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ò–ò.
    –ò–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É–µ—Ç –ª–æ–≥–∏–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤, –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤,
    –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤.
    """
    
    def __init__(self, config: Dict[str, Any]):
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AIAnalyzer (Advanced)...")
        self.config = config.get('app_config', {})
        self.cache: Dict[str, Tuple[str, float]] = {}
        
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è 'GEMINI_API_KEY' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º API —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑
        if not getattr(genai, '_configured', False):
            genai.configure(api_key=api_key)
            genai._configured = True # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

        self.model = genai.GenerativeModel('gemini-1.5-flash')
        self._ping_api()
        logger.info("AIAnalyzer —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∏ API –¥–æ—Å—Ç—É–ø–µ–Ω.")

    def _ping_api(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å API Gemini –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏."""
        try:
            timeout = self.config.get('ai_ping_timeout', 10)
            logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ API Gemini —Å —Ç–∞–π–º–∞—É—Ç–æ–º {timeout}—Å...")
            self.model.generate_content("ping", request_options={'timeout': timeout})
        except Exception as e:
            raise ConnectionError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ API Gemini: {e}") from e

    async def _get_response_with_cache(self, prompt: str, context: str, use_cache: bool = True) -> str:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ –ò–ò, –∏—Å–ø–æ–ª—å–∑—É—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        prompt_hash = hashlib.md5(prompt.encode('utf-8')).hexdigest()
        if use_cache and (cached_response := self.cache.get(prompt_hash)):
            response_text, timestamp = cached_response
            if time.time() - timestamp < self.config.get('ai_cache_ttl', 3600):
                logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è '{context}'.")
                return response_text

        logger.debug(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤ –ò–ò. –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}")
        response = await self.model.generate_content_async(prompt)
        
        if not response.parts:
            logger.warning(f"–û—Ç–≤–µ—Ç –æ—Ç –ò–ò –±—ã–ª –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω. –§–∏–¥–±–µ–∫: {response.prompt_feedback}")
            return "{}" # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π JSON, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫

        response_text = response.text
        if use_cache:
            self.cache[prompt_hash] = (response_text, time.time())
        return response_text

    @staticmethod
    def _extract_json_from_response(text: str) -> Dict:
        """–ù–∞–¥–µ–∂–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç JSON –æ–±—ä–µ–∫—Ç –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ò–ò, —É–¥–∞–ª—è—è –æ–±–µ—Ä—Ç–∫—É ```json."""
        # –ò—â–µ–º –±–ª–æ–∫ JSON, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–∫–ª—é—á–µ–Ω –≤ ```json ... ```
        match = re.search(r'```json\s*(\{.*\}|\[.*\])\s*```', text, re.DOTALL)
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –±–ª–æ–∫ –≤ ```json, –∏–∑–≤–ª–µ–∫–∞–µ–º –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        if match:
            json_text = match.group(1)
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç - —ç—Ç–æ JSON
            json_text = text

        try:
            return json.loads(json_text)
        except json.JSONDecodeError as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON. –û—à–∏–±–∫–∞: {e}. –¢–µ–∫—Å—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {json_text}")
            raise ValueError(f"JSON-–æ–±—ä–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ –ò–ò.") from e

    # --- –ú–µ—Ç–æ–¥—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ ---

    def _create_profile_prompt(self, system_data: Dict, kb_config: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        return f"""
        Analyze the user's system data to determine their primary profile from 'Gamer', 'Developer', 'Designer', 'OfficeWorker', 'HomeUser'.
        Base your decision on hardware specs, installed software keywords, and filesystem markers.
        Respond with ONLY ONE word in a JSON object: {{"profile": "..."}}.
        
        Profiler Configuration (keywords to look for):
        {json.dumps(kb_config, indent=2)}

        System Data:
        {json.dumps(system_data, indent=2, default=str)}
        """

    def _create_plan_prompt(self, system_data: Dict, profile: str, kb: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        return f"""
        You are an expert Windows optimization engineer. Your task is to create a safe and effective optimization plan.
        Your response MUST BE A SINGLE, VALID JSON OBJECT with two specific keys: "action_plan" and "cleanup_plan".

        **JSON STRUCTURE REQUIREMENTS:**
        - "action_plan": A flat list of action objects. Each object must have keys: "type", "id", "action", "reason", "user_explanation_ru".
        - "cleanup_plan": A dictionary where keys are cleanup categories from the knowledge base, and values are objects with a "clean": true/false key.

        **EXAMPLE of a valid response format:**
        {{
        "action_plan": [
            {{
            "type": "service",
            "id": "Fax",
            "action": "disable",
            "package_full_name": null,
            "reason": "Fax service is not needed for a HomeUser.",
            "user_explanation_ru": "–û—Ç–∫–ª—é—á–µ–Ω–∞ –Ω–µ–Ω—É–∂–Ω–∞—è —Å–ª—É–∂–±–∞ —Ñ–∞–∫—Å–æ–≤."
            }},
            {{
            "type": "uwp_app",
            "id": "Microsoft.YourPhone",
            "action": "remove",
            "package_full_name": "Microsoft.YourPhone_1.2.3_x64__abc",
            "reason": "Bloatware for a user without an Android phone.",
            "user_explanation_ru": "–£–¥–∞–ª–µ–Ω–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Å–≤—è–∑–∏ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–æ–º."
            }}
        ],
        "cleanup_plan": {{
            "user_temp": {{"clean": true, "description": "User temporary files"}},
            "windows_temp": {{"clean": true, "description": "System temporary files"}},
            "prefetch": {{"clean": false, "description": "Prefetch files, keep for performance."}}
        }}
        }}

        USER PROFILE: {profile}

        KNOWLEDGE BASE (contains safety rules and recommendations):
        {json.dumps(kb.get("absolutely_critical", {}), indent=2)}

        SYSTEM SNAPSHOT (data to analyze):
        {json.dumps(system_data, indent=2, default=str)}

        Strictly follow all safety rules. Do not touch critical items. Provide ONLY the JSON object as a response.
        """

    def _create_report_prompt(self, summary: Dict, plan: List[Dict]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞."""
        def format_bytes(b):
            gb, mb, kb = b / (1024**3), b / (1024**2), b / 1024
            if gb >= 1: return f"{gb:.2f} –ì–ë"
            if mb >= 1: return f"{mb:.1f} –ú–ë"
            if kb >= 1: return f"{kb:.1f} –ö–ë"
            return f"{b} –±–∞–π—Ç" if b > 0 else "0 –±–∞–π—Ç"
        
        cleaned_size = format_bytes(summary.get("cleanup", {}).get("cleaned_size_bytes", 0))
        debloat_summary = summary.get("debloat", {})
        actions_performed_str = "\n".join([f"‚úÖ {action['user_explanation_ru']}" for action in plan if action.get("user_explanation_ru")])

        return f"""
        You are "WinSpector AI Communicator". Your job is to create a friendly, encouraging report in Russian Markdown.
        
        CONTEXT:
        The user has just completed a system optimization. The tone should be positive and celebratory. Use simple, clear language.
        
        METRICS:
        - Space freed: {cleaned_size}
        - Services disabled: {len(debloat_summary.get("disabled_services", []))}
        - UWP apps removed: {len(debloat_summary.get("removed_apps", []))}
        
        ACTIONS PERFORMED:
        {actions_performed_str}
        
        TASK:
        Create a concise, well-formatted report in Russian Markdown with an encouraging headline, a summary of key metrics, a list of actions, and a reassuring closing statement. Use emojis (‚úÖ, üöÄ, üí™).
        """

    def _create_suggestions_prompt(self, **kwargs) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é."""
        # –≠—Ç–æ—Ç –ø—Ä–æ–º–ø—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π, –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –æ–Ω —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∑–¥–µ—Å—å
        return f"""
        You are "WinSpector AI Architect", a lead developer reviewing an optimization session.
        Your goal is to suggest future improvements.
        
        SESSION ANALYSIS:
        {json.dumps(kwargs, indent=2, ensure_ascii=False)}
        
        TASK:
        Based on this session's data, suggest 3-5 concrete improvements for future versions.
        Respond in Russian Markdown.
        """

    # --- –ü—É–±–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã API ---

    async def determine_user_profile(self, system_data: Dict, kb_config: Dict) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        prompt = self._create_profile_prompt(system_data, kb_config)
        response_text = await self._get_response_with_cache(prompt, "determine_user_profile")
        try:
            profile_data = self._extract_json_from_response(response_text)
            profile = profile_data.get("profile", "HomeUser").strip()
            logger.info(f"–ò–ò –æ–ø—Ä–µ–¥–µ–ª–∏–ª –ø—Ä–æ—Ñ–∏–ª—å –∫–∞–∫: {profile}")
            return profile
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {e}")
            return "HomeUser"

    async def generate_distillation_plan(self, system_data: Dict, profile: str, kb: Dict) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –ø–ª–∞–Ω –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        prompt = self._create_plan_prompt(system_data, profile, kb)
        response_text = await self._get_response_with_cache(prompt, "generate_distillation_plan")
        
        try:
            plan = self._extract_json_from_response(response_text)
            # –¢–µ–ø–µ—Ä—å _validate_plan –Ω–µ –ø–∞–¥–∞–µ—Ç, –∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π –ø–ª–∞–Ω
            safe_plan = self._validate_plan(plan, kb) 
            logger.debug("–ü–æ–ª—É—á–µ–Ω –∏ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–ª–∞–Ω –æ—Ç –ò–ò.")
            return safe_plan
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∏–ª–∏ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞–Ω –æ—Ç –ò–ò: {e}\n–û—Ç–≤–µ—Ç –ò–ò: {response_text}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω–æ –ø—É—Å—Ç–æ–π –ø–ª–∞–Ω
            return {"action_plan": [], "cleanup_plan": {}}

    def _validate_plan(self, plan: Dict, kb: Dict) -> Dict:
        """
        –ü—Ä–æ–≤–æ–¥–∏—Ç —Å—Ç—Ä–æ–≥—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø–ª–∞–Ω–∞ –æ—Ç –ò–ò.
        –ù–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è —É–¥–∞–ª—è—é—Ç—Å—è –∏–∑ –ø–ª–∞–Ω–∞, –∞ –Ω–µ –≤—ã–∑—ã–≤–∞—é—Ç –æ—à–∏–±–∫—É.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π, –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–ª–∞–Ω.
        """
        if not isinstance(plan, dict) or "action_plan" not in plan or "cleanup_plan" not in plan:
            raise ValueError("–ü–ª–∞–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º —Å –∫–ª—é—á–∞–º–∏ 'action_plan' –∏ 'cleanup_plan'.")

        action_plan = plan.get("action_plan", [])
        if not isinstance(action_plan, list):
            raise ValueError(f"'action_plan' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º, –∞ –Ω–µ {type(action_plan).__name__}.")

        critical_services = {s.lower() for s in kb.get("absolutely_critical", {}).get("services", [])}
        critical_uwp_apps = {a.lower() for a in kb.get("absolutely_critical", {}).get("uwp_apps", [])}

        safe_actions = []
        for action in action_plan:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            if not isinstance(action, dict) or not {"type", "id", "action"}.issubset(action.keys()):
                logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –≤ –ø–ª–∞–Ω–µ: {action}")
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å
            item_id_lower = str(action["id"]).lower()
            is_unsafe = False
            if action["type"] == "service" and item_id_lower in critical_services:
                logger.warning(f"–û–¢–ö–õ–û–ù–ï–ù–û –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞–¥ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π —Å–ª—É–∂–±–æ–π: {action['id']}")
                is_unsafe = True
            if action["type"] == "uwp_app" and item_id_lower in critical_uwp_apps:
                logger.warning(f"–û–¢–ö–õ–û–ù–ï–ù–û –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞–¥ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º UWP-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º: {action['id']}")
                is_unsafe = True
            
            if not is_unsafe:
                safe_actions.append(action)

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–ª–∞–Ω —Ç–æ–ª—å–∫–æ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏
        plan["action_plan"] = safe_actions
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è cleanup_plan –æ—Å—Ç–∞–µ—Ç—Å—è
        cleanup_plan = plan.get("cleanup_plan", {})
        if not isinstance(cleanup_plan, dict):
            raise ValueError(f"'cleanup_plan' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º, –∞ –Ω–µ {type(cleanup_plan).__name__}.")

        logger.info(f"–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–¥–æ–±—Ä–µ–Ω–æ {len(safe_actions)} –¥–µ–π—Å—Ç–≤–∏–π.")
        return plan

    async def generate_final_report(self, summary: Dict, plan: List[Dict]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞.")
        prompt = self._create_report_prompt(summary, plan)
        return await self._get_response_with_cache(prompt, "generate_final_report", use_cache=False)

    async def get_ai_suggestions_for_improvement(self, **kwargs) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Å—Å–∏—é –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤."""
        logger.info("–ó–∞–ø—Ä–æ—Å –∫ –ò–ò –Ω–∞ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—é –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é.")
        prompt = self._create_suggestions_prompt(**kwargs)
        # –≠—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–≤–µ–∂–∏–º, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à
        response = await self.model.generate_content_async(prompt)
        return response.text